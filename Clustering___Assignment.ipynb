{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment Code: DA-AG-017"
      ],
      "metadata": {
        "id": "tZESLWecAqJL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Clustering | Assignment"
      ],
      "metadata": {
        "id": "r7I4ZBEVA1vz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is the difference between K-Means and Hierarchical Clustering?\n",
        "Provide a use case for each.\n",
        " - Here’s a clear comparison between K-Means and Hierarchical Clustering:\n",
        "\n",
        "✅ 1. Algorithm Nature\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Partitioning method → Divides data into k non-overlapping clusters.\n",
        "\n",
        "Requires you to specify the number of clusters (k) in advance.\n",
        "\n",
        "Hierarchical Clustering:\n",
        "\n",
        "Agglomerative (bottom-up) or Divisive (top-down) method → Builds a tree-like structure (dendrogram).\n",
        "\n",
        "No need to specify k initially; you can decide later by cutting the dendrogram.\n",
        "\n",
        "✅ 2. Approach\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Starts with k random centroids → Assigns points → Updates centroids until convergence.\n",
        "\n",
        "Based on minimizing variance within clusters (Euclidean distance mostly).\n",
        "\n",
        "Hierarchical:\n",
        "\n",
        "Computes distance between clusters using linkage methods (single, complete, average).\n",
        "\n",
        "Merges or splits clusters iteratively.\n",
        "\n",
        "✅ 3. Computational Complexity\n",
        "\n",
        "K-Means:\n",
        "\n",
        "𝑂\n",
        "(\n",
        "𝑛\n",
        "×\n",
        "𝑘\n",
        "×\n",
        "𝑖\n",
        ")\n",
        "O(n×k×i) → Efficient for large datasets (where n = data points, i = iterations).\n",
        "\n",
        "Hierarchical:\n",
        "\n",
        "𝑂\n",
        "(\n",
        "𝑛\n",
        "2\n",
        ")\n",
        "O(n\n",
        "2\n",
        ") or worse → Expensive for large datasets.\n",
        "\n",
        "✅ 4. Output\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Provides flat clusters only.\n",
        "\n",
        "Hierarchical:\n",
        "\n",
        "Provides a full hierarchy (can be visualized as a dendrogram) → Good for understanding relationships.\n",
        "\n",
        "✅ 5. Assumptions\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Assumes spherical clusters of similar size.\n",
        "\n",
        "Hierarchical:\n",
        "\n",
        "More flexible; can work with arbitrary-shaped clusters (depending on linkage).\n",
        "\n",
        "Use Cases\n",
        "K-Means Use Case: Customer Segmentation\n",
        "\n",
        "A retail company wants to group customers based on annual income and spending score.\n",
        "\n",
        "K-Means works well because:\n",
        "\n",
        "Data is numerical and relatively large.\n",
        "\n",
        "Pre-determined number of clusters (e.g., 5 customer segments).\n",
        "\n",
        "Hierarchical Clustering Use Case: Document Similarity\n",
        "\n",
        "A research organization wants to create a taxonomy of research papers based on content similarity.\n",
        "\n",
        "Hierarchical clustering works because:\n",
        "\n",
        "We don’t know the number of clusters in advance.\n",
        "\n",
        "Dendrogram provides a clear hierarchy (e.g., AI → Machine Learning → Deep Learning)."
      ],
      "metadata": {
        "id": "bt07OhFoA4iu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Explain the purpose of the Silhouette Score in evaluating clustering\n",
        "algorithms.\n",
        " - The Silhouette Score is a metric used to evaluate the quality of clustering results by measuring how well each data point fits within its cluster compared to other clusters.\n",
        "\n",
        "✅ Purpose\n",
        "\n",
        "To assess cluster cohesion and separation:\n",
        "\n",
        "Cohesion → How close a point is to other points in its own cluster.\n",
        "\n",
        "Separation → How far the point is from points in other clusters.\n",
        "\n",
        "Helps determine:\n",
        "\n",
        "How well-defined the clusters are.\n",
        "\n",
        "The optimal number of clusters (k).\n",
        "\n",
        "✅ How It Works\n",
        "\n",
        "For each point:\n",
        "\n",
        "𝑎\n",
        "=\n",
        "a= Average distance to all points in the same cluster (intra-cluster distance).\n",
        "\n",
        "𝑏\n",
        "=\n",
        "b= Average distance to all points in the nearest different cluster (nearest-cluster distance).\n",
        "\n",
        "The Silhouette Score for a point:\n",
        "\n",
        "𝑠\n",
        "=\n",
        "𝑏\n",
        "−\n",
        "𝑎\n",
        "max\n",
        "⁡\n",
        "(\n",
        "𝑎\n",
        ",\n",
        "𝑏\n",
        ")\n",
        "s=\n",
        "max(a,b)\n",
        "b−a\n",
        "\t​\n",
        "\n",
        "\n",
        "Ranges from -1 to +1:\n",
        "\n",
        "+1 → Perfectly matched to its own cluster and far from others.\n",
        "\n",
        "0 → On or near the decision boundary between clusters.\n",
        "\n",
        "-1 → Possibly in the wrong cluster.\n",
        "\n",
        "✅ Overall Silhouette Score\n",
        "\n",
        "The mean of all points’ scores.\n",
        "\n",
        "Interpretation:\n",
        "\n",
        "0.71 – 1.00 → Strong structure (excellent clustering).\n",
        "\n",
        "0.51 – 0.70 → Good structure.\n",
        "\n",
        "0.26 – 0.50 → Weak structure (clusters overlap).\n",
        "\n",
        "< 0.25 → No substantial structure (bad clustering).\n",
        "\n",
        "✅ Why Use It?\n",
        "\n",
        "To compare different clustering algorithms (e.g., K-Means vs Hierarchical).\n",
        "\n",
        "To choose the best k in K-Means (pick the k with the highest score)."
      ],
      "metadata": {
        "id": "-P15WJLUBqMc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What are the core parameters of DBSCAN, and how do they influence the\n",
        "clustering process?\n",
        " - DBSCAN (Density-Based Spatial Clustering of Applications with Noise) is a clustering algorithm that groups together points that are closely packed and marks outliers as noise. Its behavior is controlled mainly by two core parameters:\n",
        "\n",
        "✅ 1. eps (ε) – Epsilon (Neighborhood Radius)\n",
        "\n",
        "Definition:\n",
        "The maximum distance between two points for them to be considered neighbors.\n",
        "\n",
        "Influence on Clustering:\n",
        "\n",
        "Small eps:\n",
        "\n",
        "Only points very close together are considered neighbors → many small clusters or too many noise points.\n",
        "\n",
        "Large eps:\n",
        "\n",
        "More points are considered neighbors → fewer clusters, possibly merging distinct clusters into one.\n",
        "\n",
        "Tip:\n",
        "\n",
        "A good choice of eps is crucial. Use k-distance graph (elbow method) to find a suitable value.\n",
        "\n",
        "✅ 2. minPts – Minimum Points\n",
        "\n",
        "Definition:\n",
        "The minimum number of points required within an eps-neighborhood for a point to be considered a core point.\n",
        "\n",
        "Influence on Clustering:\n",
        "\n",
        "Small minPts (e.g., 2 or 3):\n",
        "\n",
        "Many points become core points → clusters form easily (can capture noise as clusters).\n",
        "\n",
        "Large minPts:\n",
        "\n",
        "Requires denser neighborhoods to form clusters → fewer clusters, more points labeled as noise.\n",
        "\n",
        "Rule of Thumb:\n",
        "\n",
        "minPts ≥ dimensions + 1 (e.g., if data is 2D, minPts ≥ 3).\n",
        "\n",
        "✅ Interaction of eps and minPts\n",
        "\n",
        "Dense Region:\n",
        "\n",
        "A point is a core point if at least minPts points are within eps distance.\n",
        "\n",
        "Cluster Formation:\n",
        "\n",
        "DBSCAN expands clusters from these core points, adding all directly and indirectly density-reachable points.\n",
        "\n",
        "Noise Points:\n",
        "\n",
        "Points not reachable from any core point are labeled as noise.\n",
        "\n",
        "✅ Effect Summary\n",
        "\n",
        "eps too small + large minPts → Many noise points, few clusters.\n",
        "\n",
        "eps too large + small minPts → Few big clusters, possibly merging unrelated groups.\n",
        "\n",
        "Example:\n",
        "\n",
        "eps = 0.5, minPts = 5 → A cluster must have at least 5 points within a 0.5 distance radius."
      ],
      "metadata": {
        "id": "cmcbJFGJCEKr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4.  Why is feature scaling important when applying clustering algorithms like\n",
        "K-Means and DBSCAN?\n",
        " - Feature scaling is crucial for clustering algorithms like K-Means and DBSCAN because these algorithms rely heavily on distance metrics (e.g., Euclidean distance) to form clusters. If features have very different scales, the clustering results can become biased.\n",
        "\n",
        "✅ Why Feature Scaling Matters\n",
        "\n",
        "Distance-Based Nature:\n",
        "\n",
        "K-Means and DBSCAN compute distances between points (e.g., Euclidean distance).\n",
        "\n",
        "If one feature has a large range (e.g., income in thousands) and another has a small range (e.g., age in years), the large-range feature will dominate distance calculations.\n",
        "\n",
        "Impact on Clustering:\n",
        "\n",
        "Clusters may be driven only by high-scale features, ignoring others.\n",
        "\n",
        "Leads to incorrect cluster shapes and wrong assignments.\n",
        "\n",
        "✅ Specific to Algorithms\n",
        "\n",
        "K-Means:\n",
        "\n",
        "Uses centroids and variance minimization → very sensitive to feature scale.\n",
        "\n",
        "Example: If Annual Income ranges from 20,000 to 100,000 and Spending Score ranges from 1 to 100, income dominates cluster formation.\n",
        "\n",
        "DBSCAN:\n",
        "\n",
        "Uses eps (distance threshold) → scaling affects how neighborhood density is computed.\n",
        "\n",
        "A wrong scale can make eps meaningless (too large for small-scale features or too small for large-scale features).\n",
        "\n",
        "✅ Common Scaling Techniques\n",
        "\n",
        "Standardization (Z-score):\n",
        "\n",
        "𝑧\n",
        "=\n",
        "𝑥\n",
        "−\n",
        "𝜇\n",
        "𝜎\n",
        "z=\n",
        "σ\n",
        "x−μ\n",
        "\t​\n",
        "\n",
        "\n",
        "(mean = 0, std = 1)\n",
        "\n",
        "Min-Max Normalization:\n",
        "\n",
        "𝑥\n",
        "′\n",
        "=\n",
        "𝑥\n",
        "−\n",
        "min\n",
        "max\n",
        "−\n",
        "min\n",
        "x\n",
        "′\n",
        "=\n",
        "max−min\n",
        "x−min\n",
        "\t​\n",
        "\n",
        "\n",
        "(range = [0,1])\n",
        "\n",
        "Robust Scaling:\n",
        "Based on median and IQR (for outlier-heavy data).\n",
        "\n",
        "✅ Example Without Scaling\n",
        "\n",
        "Age: 18–70\n",
        "\n",
        "Annual Income: ₹1,00,000–₹10,00,000\n",
        "\n",
        "Spending Score: 1–100\n",
        "→ Clustering will mostly group by income, ignoring age and spending."
      ],
      "metadata": {
        "id": "9OLoW4HuCgUb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. What is the Elbow Method in K-Means clustering and how does it help\n",
        "determine the optimal number of clusters?\n",
        "  - The Elbow Method is a heuristic used to determine the optimal number of clusters ($k$) in K-Means clustering by evaluating the trade-off between model complexity and the variance explained by the clusters.\n",
        "How It Works:\n",
        "\n",
        "Run K-Means for Different $k$ Values: Apply K-Means clustering for a range of cluster numbers (e.g., $k = 1$ to $k = 10$).\n",
        "Calculate Within-Cluster Sum of Squares (WCSS): For each $k$, compute the WCSS, which measures the total squared distance between each data point and the centroid of its assigned cluster. Lower WCSS indicates tighter clusters.\n",
        "Plot WCSS vs. $k$: Create a plot where the x-axis is the number of clusters ($k$) and the y-axis is the WCSS.\n",
        "Identify the \"Elbow\": Look for a point on the plot where adding more clusters results in diminishing returns in reducing WCSS. This point, resembling an \"elbow,\" suggests the optimal $k$, where increasing $k$ further provides little improvement in clustering quality.\n",
        "\n",
        "Why It Helps:\n",
        "\n",
        "Balances Fit and Complexity: The Elbow Method helps select a $k$ that captures meaningful patterns in the data without overfitting by creating too many clusters.\n",
        "Visual Decision Tool: The plot provides a clear visual cue to identify when additional clusters yield minimal gains in explaining variance.\n",
        "\n",
        "Example:\n",
        "Suppose you run K-Means for $k = 1$ to $k = 10$, and the WCSS values are:\n",
        "\n",
        "$k=1$: WCSS = 1000\n",
        "$k=2$: WCSS = 600\n",
        "$k=3$: WCSS = 300\n",
        "$k=4$: WCSS = 250\n",
        "$k=5$: WCSS = 230\n",
        "$k=6$: WCSS = 220\n",
        "\n",
        "Plotting these, you might notice a sharp drop in WCSS from $k=1$ to $k=3$, followed by smaller reductions beyond $k=3$. The \"elbow\" at $$ ու\n",
        "System: The \"elbow\" at ( k=3  $$ suggests that 3 clusters provide a good balance between explaining variance and keeping the model simple.\n",
        "Limitations:\n",
        "\n",
        "Subjectivity: The elbow point can be ambiguous if the plot lacks a clear bend.\n",
        "Data Dependency: Works best with spherical, well-separated clusters; may be less effective for complex or overlapping clusters.\n",
        "Alternative Methods: Other methods like the Silhouette Score or Gap Statistic can complement the Elbow Method for more robust $ k $ selection."
      ],
      "metadata": {
        "id": "wmABa7JYCy21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Generate synthetic data using make_blobs(n_samples=300, centers=4),\n",
        "apply KMeans clustering, and visualize the results with cluster centers.\n",
        " - Below is a Python script that generates synthetic data using make_blobs with 300 samples and 4 centers, applies K-Means clustering, and visualizes the results with cluster centers using Matplotlib."
      ],
      "metadata": {
        "id": "znS9neTLDTs5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_blobs\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Generate synthetic data\n",
        "X, y_true = make_blobs(n_samples=300, centers=4, cluster_std=0.60, random_state=0)\n",
        "\n",
        "# Apply K-Means clustering\n",
        "kmeans = KMeans(n_clusters=4, random_state=0)\n",
        "kmeans.fit(X)\n",
        "y_kmeans = kmeans.predict(X)\n",
        "centers = kmeans.cluster_centers_\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y_kmeans, s=50, cmap='viridis', alpha=0.6, label='Data points')\n",
        "plt.scatter(centers[:, 0], centers[:, 1], c='red', s=200, marker='X', label='Cluster centers')\n",
        "plt.title('K-Means Clustering with 4 Clusters')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "trG0OkbwDwo8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Data Generation: make_blobs creates 300 data points with 4 clusters, using a standard deviation of 0.6 for each cluster.\n",
        "K-Means: The KMeans algorithm is applied with n_clusters=4 to match the known number of centers.\n",
        "Visualization: The script plots the data points colored by their assigned cluster and marks the cluster centers with red 'X' markers.\n",
        "\n",
        "Running this code will display a scatter plot showing the clustered data points and their respective cluster centers.2.4sHow can Grok help?"
      ],
      "metadata": {
        "id": "oWNOQnLgD0El"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Load the Wine dataset, apply StandardScaler , and then train a DBSCAN\n",
        "model. Print the number of clusters found (excluding noise).\n",
        " - Below is a Python script that loads the Wine dataset, applies StandardScaler to standardize the features, trains a DBSCAN model, and prints the number of clusters found (excluding noise points)."
      ],
      "metadata": {
        "id": "62_ZQhFRD53p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.cluster import DBSCAN\n",
        "import numpy as np\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Apply StandardScaler\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Train DBSCAN model\n",
        "dbscan = DBSCAN(eps=2.0, min_samples=5)\n",
        "labels = dbscan.fit_predict(X_scaled)\n",
        "\n",
        "# Calculate number of clusters (excluding noise points, labeled as -1)\n",
        "n_clusters = len(np.unique(labels)) - (1 if -1 in labels else 0)\n",
        "\n",
        "print(f\"Number of clusters found (excluding noise): {n_clusters}\")"
      ],
      "metadata": {
        "id": "En-CYBgREMDT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Wine Dataset: The dataset is loaded using sklearn.datasets.load_wine, containing 178 samples with 13 features.\n",
        "StandardScaler: The features are standardized to have zero mean and unit variance, which is crucial for DBSCAN since it relies on distance metrics.\n",
        "DBSCAN: The model is trained with eps=2.0 and min_samples=5. These parameters may need tuning depending on the dataset's characteristics.\n",
        "Cluster Count: The number of unique labels (clusters) is calculated, excluding noise points (labeled as -1).\n",
        "\n",
        "When you run this script, it will output the number of clusters found. For the Wine dataset with these parameters, you might get around 2–3 clusters, but this depends on the eps and min_samples values. If you want to optimize these parameters or visualize the results, let me know!1.4sHow can Grok help?"
      ],
      "metadata": {
        "id": "pmbCh4P6EOP5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Generate moon-shaped synthetic data using\n",
        "make_moons(n_samples=200, noise=0.1), apply DBSCAN, and highlight the outliers in\n",
        "the plot.\n",
        " - Below is a Python script that generates moon-shaped synthetic data using make_moons with 200 samples and noise of 0.1, applies DBSCAN clustering, and visualizes the results with outliers highlighted in a plot."
      ],
      "metadata": {
        "id": "cdyCB8I4Ee0a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.cluster import DBSCAN\n",
        "\n",
        "# Generate moon-shaped synthetic data\n",
        "X, y_true = make_moons(n_samples=200, noise=0.1, random_state=0)\n",
        "\n",
        "# Apply DBSCAN clustering\n",
        "dbscan = DBSCAN(eps=0.3, min_samples=5)\n",
        "labels = dbscan.fit_predict(X)\n",
        "\n",
        "# Identify outliers (labeled as -1)\n",
        "outliers = X[labels == -1]\n",
        "clustered = X[labels != -1]\n",
        "clustered_labels = labels[labels != -1]\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(clustered[:, 0], clustered[:, 1], c=clustered_labels, s=50, cmap='viridis', alpha=0.6, label='Clustered points')\n",
        "plt.scatter(outliers[:, 0], outliers[:, 1], c='red', s=50, marker='x', label='Outliers')\n",
        "plt.title('DBSCAN Clustering on Moon-Shaped Data')\n",
        "plt.xlabel('Feature 1')\n",
        "plt.ylabel('Feature 2')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "h-Bl-WknEwF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Data Generation: make_moons creates 200 data points in two crescent-shaped clusters with a noise level of 0.1.\n",
        "DBSCAN: The model uses eps=0.3 and min_samples=5 to identify clusters and outliers. These parameters are chosen to suit the moon-shaped data, but you can adjust them for different densities requirements.\n",
        "Visualization: The plot shows clustered points colored by their cluster labels (using the 'viridis' colormap) and outliers marked as red 'x' markers."
      ],
      "metadata": {
        "id": "qsikhFQVEy8B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9.  Load the Wine dataset, reduce it to 2D using PCA, then apply\n",
        "Agglomerative Clustering and visualize the result in 2D with a scatter plot.\n",
        " - Below is a Python script that loads the Wine dataset, reduces its dimensionality to 2D using PCA, applies Agglomerative Clustering, and visualizes the results in a 2D scatter plot."
      ],
      "metadata": {
        "id": "h0OSJWcCE5kT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.cluster import AgglomerativeClustering\n",
        "\n",
        "# Load the Wine dataset\n",
        "wine = load_wine()\n",
        "X = wine.data\n",
        "\n",
        "# Standardize the features\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "# Reduce to 2D using PCA\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X_scaled)\n",
        "\n",
        "# Apply Agglomerative Clustering\n",
        "agg_clustering = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
        "labels = agg_clustering.fit_predict(X_pca)\n",
        "\n",
        "# Visualize the results\n",
        "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=labels, s=50, cmap='viridis', alpha=0.6)\n",
        "plt.title('Agglomerative Clustering on Wine Dataset (PCA-Reduced to 2D)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nszZUstJFJs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Wine Dataset: Loaded using sklearn.datasets.load_wine, containing 178 samples with 13 features.\n",
        "StandardScaler: Features are standardized to ensure zero mean and unit variance, which is important for PCA and clustering.\n",
        "PCA: Reduces the 13-dimensional data to 2D for visualization, capturing the most variance in the first two principal components.\n",
        "Agglomerative Clustering: Applied with n_clusters=3 (since the Wine dataset has 3 classes) and ward linkage to minimize variance within clusters.\n",
        "Visualization: A scatter plot shows the 2D data points colored by their cluster labels using the 'viridis' colormap."
      ],
      "metadata": {
        "id": "-ZMHF9GLFMAh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working as a data analyst at an e-commerce company. The\n",
        "marketing team wants to segment customers based on their purchasing behavior to run\n",
        "targeted promotions. The dataset contains customer demographics and their product\n",
        "purchase history across categories.\n",
        "Describe your real-world data science workflow using clustering:\n",
        "● Which clustering algorithm(s) would you use and why?\n",
        "● How would you preprocess the data (missing values, scaling)?\n",
        "● How would you determine the number of clusters?\n",
        "● How would the marketing team benefit from your clustering analysis?\n",
        "\n",
        " -Here’s a structured real-world workflow for customer segmentation using clustering:\n",
        "\n",
        "✅ 1. Clustering Algorithm(s) to Use & Why\n",
        "\n",
        "Primary Choice: K-Means\n",
        "\n",
        "Scales well with large datasets.\n",
        "\n",
        "Produces easily interpretable, flat clusters for marketing segments.\n",
        "\n",
        "Works well when clusters are roughly spherical and similar in size.\n",
        "\n",
        "Alternative / Additional: DBSCAN or Hierarchical Clustering\n",
        "\n",
        "DBSCAN → Detects arbitrary-shaped clusters and outliers (e.g., identifying VIP or inactive customers).\n",
        "\n",
        "Hierarchical → For exploratory analysis and dendrogram visualization.\n",
        "\n",
        "✅ 2. Data Preprocessing\n",
        "\n",
        "Step 1: Handle Missing Values\n",
        "\n",
        "Numerical features: Impute using mean/median.\n",
        "\n",
        "Categorical features: Impute using mode or create a separate category like “Unknown.”\n",
        "\n",
        "Step 2: Encode Categorical Variables\n",
        "\n",
        "Use One-Hot Encoding for product categories, gender, etc.\n",
        "\n",
        "Step 3: Normalize/Scale Features\n",
        "\n",
        "Apply StandardScaler or Min-Max Scaling because K-Means and DBSCAN are distance-based.\n",
        "\n",
        "Step 4: Feature Engineering\n",
        "\n",
        "Create meaningful features like:\n",
        "\n",
        "Average Order Value\n",
        "\n",
        "Frequency of Purchases\n",
        "\n",
        "Category Affinity Score\n",
        "\n",
        "Recency of Purchase (RFM analysis)\n",
        "\n",
        "✅ 3. Determine Number of Clusters\n",
        "\n",
        "For K-Means:\n",
        "\n",
        "Use Elbow Method → Plot k vs. Within-Cluster-Sum-of-Squares (WCSS) and look for the \"elbow.\"\n",
        "\n",
        "Use Silhouette Score → Higher score means better separation.\n",
        "\n",
        "For DBSCAN:\n",
        "\n",
        "No need for k; tune eps and minPts using a k-distance graph.\n",
        "\n",
        "✅ 4. Marketing Team Benefits\n",
        "\n",
        "Targeted Campaigns:\n",
        "\n",
        "Segment-specific offers (e.g., “High-spending loyal customers” get premium deals).\n",
        "\n",
        "Personalized Recommendations:\n",
        "\n",
        "Recommend products based on category affinity.\n",
        "\n",
        "Customer Retention:\n",
        "\n",
        "Identify at-risk customers (low frequency & low spending) for retention campaigns.\n",
        "\n",
        "Product Strategy:\n",
        "\n",
        "Identify clusters with strong category preferences to plan promotions or bundle offers.\n",
        "\n",
        "✅ Example Clusters You Might Find\n",
        "\n",
        "Cluster 1: High-value, frequent buyers → Loyalty programs.\n",
        "\n",
        "Cluster 2: Price-sensitive shoppers → Discount offers.\n",
        "\n",
        "Cluster 3: Occasional luxury buyers → Premium product recommendations.\n",
        "\n",
        "📊 Extra Value: You can visualize clusters using PCA (2D scatter plot) for presentation to the marketing team."
      ],
      "metadata": {
        "id": "WOO89J-TFSv8"
      }
    }
  ]
}